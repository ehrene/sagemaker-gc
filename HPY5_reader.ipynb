{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the outputted model files for an example model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "This notebook follows thru the example presented above and digs into the viewing/accessing the outputted model files generated into HDF5 and JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import tensorflow\n",
    "import numpy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.4\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(numpy.__version__)\n",
    "print(tensorflow.__version__)\n",
    "#!pip install h5pyViewer\n",
    "# https://stackoverflow.com/questions/51173580/how-to-read-keras-model-weights-without-a-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0823 12:56:02.043327  9104 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0823 12:56:02.114330  9104 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0823 12:56:02.138327  9104 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0823 12:56:02.199330  9104 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0823 12:56:02.305327  9104 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0823 12:56:02.310324  9104 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0823 12:56:02.515323  9104 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17172040400>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 77.86%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"pima_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights(\"pima_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('pima_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"pima_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at HDF5 model parameter structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_1', 'dense_2', 'dense_3']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "f=h5py.File('pima_model.h5', 'r') #import the model file with 'read' permissions\n",
    "\n",
    "print(list(f.keys()))\n",
    "#dset = f['dense_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemsViewHDF5(<Attributes of HDF5 object at 1586776884416>)\n"
     ]
    }
   ],
   "source": [
    "d=f['dense_1']\n",
    "print(f.attrs.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemsViewHDF5(<Attributes of HDF5 object at 1586776885384>)\n",
      "  backend: b'tensorflow'\n",
      "  keras_version: b'2.2.5'\n",
      "  layer_names: [b'dense_1' b'dense_2' b'dense_3']\n"
     ]
    }
   ],
   "source": [
    "print(f.attrs.items())\n",
    "#if len(f.attrs.items()):\n",
    "   # print(\"{} contains: \".format(weight_file_path))\n",
    "    #print(\"Root attributes:\")\n",
    "    #print(\"  f.attrs.items(): \")\n",
    "    \n",
    "for key, value in f.attrs.items():\n",
    "    #print(key, \"  \", value)\n",
    "    print(\"  {}: {}\".format(key, value))\n",
    "\n",
    "    if len(f.items())==0:\n",
    "        print(\"  Terminate # len(f.items())==0: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dense_1\n",
      "    g.attrs.items(): Attributes:\n",
      "      weight_names: [b'dense_1/kernel:0' b'dense_1/bias:0']\n",
      "    Dataset:\n",
      "    Dataset: param.keys():\n",
      "      dense_1/bias:0: [-1.7412677   0.          1.207784   -0.16921678  1.0718697   1.2052598\n",
      " -0.9765404   1.1325557   1.41263     0.49467513 -1.5568869   0.        ]\n",
      "      dense_1/kernel:0: [[ 0.23350556  0.01882353 -0.3646723  -0.30434564 -0.15309867 -0.31964445\n",
      "   0.1690051  -0.33440226 -0.41950876 -0.5037846   0.17060773 -0.035793  ]\n",
      " [ 0.09010825 -0.03511642  0.02143405  0.08531553  0.015311   -0.04998917\n",
      "  -0.11017638  0.06727614  0.04060419 -0.1062217   0.12676387 -0.01599389]\n",
      " [-0.07434946 -0.00493143  0.09144411  0.17805102  0.10590395  0.18729377\n",
      "  -0.00898929  0.05190195  0.07317782  0.17975095 -0.07113796 -0.04335348]\n",
      " [ 0.05266909 -0.01141974  0.07563986  0.1091879  -0.05200028 -0.17930453\n",
      "   0.07660628 -0.01942675  0.0621646   0.01402787 -0.05933642  0.02202759]\n",
      " [-0.03714687 -0.00778842 -0.04004614  0.01483318 -0.0241155   0.04490931\n",
      "   0.10264199 -0.07450794 -0.05575414 -0.02019947 -0.03778779 -0.0114973 ]\n",
      " [ 0.16640294 -0.01866753 -0.15087461  0.17700724 -0.21325007 -0.09707131\n",
      "  -0.12252605 -0.08857773 -0.07560238 -0.03443523 -0.02099758  0.01339575]\n",
      " [ 1.1930578   0.00657737 -0.87800455 -0.22564949 -0.6178989  -0.23231445\n",
      "   0.8127119  -0.75114155 -0.7803015  -0.2596657   1.243664    0.00195978]\n",
      " [-0.1514379  -0.04497098 -0.10851803 -0.37707254 -0.08493323 -0.28988478\n",
      "   0.24437162 -0.2007018  -0.17803897  0.11273023  0.07095855 -0.00844307]]\n",
      "  dense_2\n",
      "    g.attrs.items(): Attributes:\n",
      "      weight_names: [b'dense_2/kernel:0' b'dense_2/bias:0']\n",
      "    Dataset:\n",
      "    Dataset: param.keys():\n",
      "      dense_2/bias:0: [ 0.         1.4132878 -1.3538951  1.4716297  1.468445  -0.0726694\n",
      "  0.         1.3320578]\n",
      "      dense_2/kernel:0: [[ 1.71101578e-02 -4.17678021e-02  3.76104772e-01 -8.70769545e-02\n",
      "  -9.16202217e-02 -5.00519387e-02  1.51272155e-02  4.51728180e-02]\n",
      " [-3.75826843e-02  4.08672728e-02  1.23197436e-02  3.47535349e-02\n",
      "  -1.53470151e-02 -4.85982075e-02 -3.78578678e-02  4.47819568e-02]\n",
      " [-4.00235876e-02  6.81143925e-02  1.16555933e-02  1.00378670e-01\n",
      "   1.67803783e-02  7.43910577e-03  4.59073298e-02  7.61535093e-02]\n",
      " [-4.60369959e-02  3.59026566e-02  2.87887186e-01  3.05115618e-02\n",
      "   1.83877703e-02 -2.94208899e-02 -1.46492235e-02  8.52493122e-02]\n",
      " [-4.25389521e-02  6.17669038e-02  7.58837759e-02  1.05782889e-01\n",
      "   5.85223362e-02 -1.07741958e-04 -4.49290648e-02 -5.00911940e-03]\n",
      " [-2.88340207e-02  2.07280263e-01 -1.71112791e-01  1.50369778e-01\n",
      "   2.05030337e-01 -7.73736015e-02 -1.83509365e-02  1.93261102e-01]\n",
      " [-2.94184685e-02 -6.46597147e-02  1.24641664e-01 -6.28828630e-02\n",
      "  -3.35832201e-02 -8.61394033e-03 -4.41738963e-02 -3.23079467e-01]\n",
      " [-3.85513157e-03  5.96644543e-02  1.52426034e-01  4.14305441e-02\n",
      "   1.11665159e-01 -9.44818705e-02  1.39489509e-02  1.31400034e-01]\n",
      " [-7.02880695e-03  6.70821592e-02  7.73968622e-02  1.33334443e-01\n",
      "   1.21819116e-01 -8.80248696e-02 -4.11297008e-03  7.31302649e-02]\n",
      " [-1.03472471e-02 -1.57721620e-02  2.37839401e-01  4.08008173e-02\n",
      "  -1.01697460e-01 -1.59560214e-03 -4.00178209e-02 -1.27381608e-02]\n",
      " [-4.59223650e-02 -1.48446739e-01 -1.95815396e-02 -1.21797532e-01\n",
      "  -1.25983030e-01 -6.93875179e-02 -3.84182446e-02 -1.55984938e-01]\n",
      " [ 4.04942036e-03  4.97518294e-02  6.92932680e-03  1.95994265e-02\n",
      "  -4.20826785e-02 -2.31025219e-02 -2.86299828e-02  2.99803950e-02]]\n",
      "  dense_3\n",
      "    g.attrs.items(): Attributes:\n",
      "      weight_names: [b'dense_3/kernel:0' b'dense_3/bias:0']\n",
      "    Dataset:\n",
      "    Dataset: param.keys():\n",
      "      dense_3/bias:0: [-1.2899107]\n",
      "      dense_3/kernel:0: [[-0.01677337]\n",
      " [-0.46028325]\n",
      " [ 0.46416754]\n",
      " [-0.45751038]\n",
      " [-0.44195133]\n",
      " [ 0.00544877]\n",
      " [ 0.02595795]\n",
      " [-0.5379912 ]]\n"
     ]
    }
   ],
   "source": [
    "for layer, g in f.items():            \n",
    "            print(\"  {}\".format(layer))\n",
    "            print(\"    g.attrs.items(): Attributes:\")\n",
    "            for key, value in g.attrs.items():\n",
    "                print(\"      {}: {}\".format(key, value))\n",
    "\n",
    "            print(\"    Dataset:\")\n",
    "            for p_name in g.keys():\n",
    "                param = g[p_name]\n",
    "                subkeys = param.keys()\n",
    "                print(\"    Dataset: param.keys():\")\n",
    "                for k_name in param.keys():\n",
    "                    print(\"      {}/{}: {}\".format(p_name, k_name, param.get(k_name)[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 77.86%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
